---
title: "BFGS Optimization"
date: "2024-01-15"
category: "AI General"
tags: ["Optimization"]
featured: true
excerpt: "The basics of overview of gradient gradient descent"
author: "Sriram"
---

# BFGS Optimization: Mathematical Concepts Simplified

## 1. Gradient Descent - The Starting Point

Gradient descent is like walking downhill to find the lowest point of a mountain.

**The Update Rule:**
$$x_{k+1} = x_k - \\alpha \\nabla f(x_k)$$

**What this means:**

- $x_k$ is your current position
- $\\nabla f(x_k)$ is the gradient (steepest slope direction)
- $\\alpha$ is the step size (how big steps you take)
- We subtract the gradient because we want to go **downhill** (negative direction)

**Key insight:** The gradient points uphill, so negative gradient points downhill - that's where we want to go!

## 2. Newton's Method - Using Curvature Information

Imagine you're not just looking at the slope, but also how the slope is changing (is it getting steeper or flatter?).

**Taylor Series Approximation (2nd order):**
$$f(x_k + \\epsilon) \\approx f(x_k) + f'(x_k)\\epsilon + \\frac{1}{2}f''(x_k)\\epsilon^2$$

**Finding the minimum:** Take the derivative and set it to zero:
$$\\frac{d}{d\\epsilon}[f(x_k) + f'(x_k)\\epsilon + \\frac{1}{2}f''(x_k)\\epsilon^2] = 0$$
$$f'(x_k) + f''(x_k)\\epsilon = 0$$
$$\\epsilon = -\\frac{f'(x_k)}{f''(x_k)}$$

**Newton's Update Rule (multidimensional):**
$$x_{k+1} = x_k - H^{-1}_k \\nabla f(x_k)$$

**What this means:**

- $\\nabla f(x_k)$ is the gradient (first derivative in multiple dimensions)
- $H_k$ is the Hessian matrix (second derivatives in multiple dimensions)
- $H^{-1}_k$ is the inverse of the Hessian

**The Hessian Matrix:**

$$
H = \\begin{bmatrix}
\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots \\\\
\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots \\\\
\\vdots & \\vdots & \\ddots
\\end{bmatrix}
$$

**Key insight:** Newton's method is much faster but very expensive to compute!

## 3. Quasi-Newton Methods - The Best of Both Worlds

The idea: Instead of computing the exact Hessian, approximate it!

**Basic Formula:**
$$x_{k+1} = x_k - B_k^{-1} \\nabla f(x_k)$$

Where $B_k$ is our **approximation** of the Hessian.

**The Quasi-Newton Condition (Secant Equation):**
$$B_{k+1} \\Delta x_k = y_k$$

Where:

- $\\Delta x_k = x_{k+1} - x_k$ (the step we just took)
- $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ (how the gradient changed)

**What this means:** The approximate Hessian should satisfy the same relationship as the true Hessian would.

**In matrix form:**
$$B_{k+1} (x_{k+1} - x_k) = \\nabla f(x_{k+1}) - \\nabla f(x_k)$$

## 4. The Underdetermined Problem

**The Challenge:** In n dimensions:

- Hessian has $\\frac{n(n+1)}{2}$ unique elements (it's symmetric)
- But we only have n equations from the quasi-Newton condition
- So we need **additional constraints** to solve for B!

## 5. BFGS - The Solution

BFGS (Broyden-Fletcher-Goldfarb-Shanno) cleverly solves the underdetermined problem by adding smart constraints. Here's how:

### The Core Insight

Since we have more unknowns than equations, BFGS adds these constraints:

1. **Preserve symmetry** and **positive-definiteness**
2. **Minimize the change** between $B_{k+1}$ and $B_k$

**Why these constraints?**

- **Symmetry**: The true Hessian is always symmetric, so our approximation should be too
- **Positive-definiteness**: For convex functions, the Hessian is positive-definite (this ensures we're moving toward a minimum, not a maximum)
- **Minimal change**: We want to update our approximation smoothly, not drastically

### The Mathematical Formulation

**The Optimization Problem:**
$\\min_{B_{k+1}} ||B_{k+1} - B_k||_F^2$
Subject to:

- $B_{k+1} \\Delta x_k = y_k$ (quasi-Newton condition)
- $B_{k+1}$ is symmetric and positive-definite

**What this means in plain English:**
"Find the new Hessian approximation $B_{k+1}$ that is as close as possible to the previous one $B_k$, while still satisfying the quasi-Newton condition."

### Working with the Inverse (More Practical)

In practice, we actually work with $H = B^{-1}$ (the inverse Hessian) because:

- Newton's method requires $B^{-1}$, not $B$
- Computing $B^{-1}$ directly is more efficient than computing $B$ and then inverting it

**Alternative formulation (for the inverse):**
$\\min_{H_{k+1}} ||H_{k+1} - H_k||_F^2$
Subject to:

- $H_{k+1} y_k = \\Delta x_k$ (inverted quasi-Newton condition)
- $H_{k+1}$ is symmetric

Where $H_k = B_k^{-1}$

### Understanding the Frobenius Norm

The **Frobenius norm** measures how "different" two matrices are:
$||A||_F = \\sqrt{\\sum_{i,j} |A_{ij}|^2}$

**Think of it as:** The square root of the sum of squares of all matrix elements. It's like the Euclidean distance but for matrices.

**Why this norm?** Different norms lead to different quasi-Newton methods. The Frobenius norm was chosen for BFGS because:

- It's mathematically tractable
- It leads to a computationally efficient solution
- It preserves positive-definiteness

### The Inverted Quasi-Newton Condition

Notice how the condition changes when we work with the inverse:

- **Original**: $B_{k+1} \\Delta x_k = y_k$
- **Inverted**: $H_{k+1} y_k = \\Delta x_k$

This comes from the fact that if $B \\Delta x = y$, then $B^{-1} y = \\Delta x$.

**Physical interpretation:** The inverse Hessian tells us how much we should move ($\\Delta x$) given a change in gradient ($y$).

### Why This Approach Works

1. **Uniqueness**: These constraints provide just enough information to uniquely determine $H_{k+1}$
2. **Efficiency**: We avoid computing the full Hessian while still capturing second-order information
3. **Stability**: The minimal change principle ensures the algorithm remains stable
4. **Convergence**: The resulting updates lead to superlinear convergence for well-behaved functions

This optimization problem, while complex to state, leads to a surprisingly elegant closed-form solution that we'll see in the next section.

## 6. The BFGS Update Formula

**Rank-Two Update:**
$$B_{k+1} = B_k + U_k + V_k$$

Where $U_k$ and $V_k$ are rank-one matrices of the form:

- $U_k = a_k u_k u_k^T$
- $V_k = b_k v_k v_k^T$

**Final BFGS Update for B:**
$$B_{k+1} = B_k + \\frac{y_k y_k^T}{y_k^T \\Delta x_k} - \\frac{B_k \\Delta x_k \\Delta x_k^T B_k}{\\Delta x_k^T B_k \\Delta x_k}$$

**BFGS Update for B⁻¹ (what we actually compute):**

$$B_{k+1}^{-1} = \\left(I - \\frac{\\Delta x_k y_k^T}{y_k^T \\Delta x_k}\\right) B_k^{-1} \\left(I - \\frac{y_k \\Delta x_k^T}{y_k^T \\Delta x_k}\\right) + \\frac{\\Delta x_k \\Delta x_k^T}{y_k^T \\Delta x_k}$$

## 7. Understanding the BFGS Update Components

**First term removal:**
$$- \\frac{B_k \\Delta x_k \\Delta x_k^T B_k}{\\Delta x_k^T B_k \\Delta x_k}$$
This **removes** information from the old approximation in the direction of $\\Delta x_k$.

**Second term addition:**
$$+ \\frac{y_k y_k^T}{y_k^T \\Delta x_k}$$
This **adds** new information in the direction of $y_k$.

**Key insight:** BFGS carefully balances old and new information to create the best approximation.

## 8. Why Rank-Two Updates?

**Positive-Definiteness Preservation:**
For any vector $z$, if $B_k^{-1}$ is positive-definite:
$$z^T B_{k+1}^{-1} z = z^T B_k^{-1} z - \\frac{(\\Delta x_k^T z)^2}{y_k^T \\Delta x_k} + \\frac{(y_k^T z)^2}{y_k^T \\Delta x_k}$$

Both terms are non-negative (given the curvature condition $y_k^T \\Delta x_k > 0$), so positive-definiteness is preserved.

**Key insight:** Rank-one updates don't preserve positive-definiteness, but rank-two updates do!

## 9. Algorithm Summary

1. **Initialize:** Set $B_0^{-1}$ (often to identity matrix $I$)
2. **For each iteration k:**
   - Compute $x_{k+1} = x_k - B_k^{-1} \\nabla f(x_k)$
   - Calculate $\\Delta x_k = x_{k+1} - x_k$
   - Calculate $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$
   - Update $B_{k+1}^{-1}$ using the BFGS formula
3. **Repeat** until convergence

## 10. Why BFGS Works

**Efficiency:**

- Avoids computing the actual Hessian ($O(n^3)$ operation)
- Only requires gradients and previous information

**Convergence:**

- Faster than gradient descent (uses curvature information)
- More stable than Newton's method

**Memory:**

- Only stores one $n \\times n$ matrix instead of computing Hessian each time

The beauty of BFGS is that it approximates the benefits of Newton's method while avoiding its computational costs!


