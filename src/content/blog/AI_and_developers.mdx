---
title: "Keep Experienced Developers"
date: "2025-08-15"
category: "AI"
tags: ["AI", "Layoff"]
featured: true
excerpt: "Keep the Developer"
author: "Sriram"
---


# A Practical Appeal: Keep Experienced Developers to Secure AI Success, Not Cut Them in Mass Layoffs

I’m writing this blog to support developers who are experiencing software layoffs and to respectfully urge companies to understand the situation and recognize that developers may contribute far more value than is apparent when making decisions to let them go.

## Summary

**I(We) fully support AI advancement** - the technology is remarkable and will transform everying including software development. However, current evidence suggests we are in a critical transition period where experienced developers are more essential than ever **as valuable testers, validators, and AI improvement partners**. The massive volume of AI-generated code requires proportional validation capacity to prevent data corruption that could undermine future AI development. This blog presents a practical case for maintaining development expertise to support AI success, particularly for validation processes that will determine whether AI improves or degrades over time.

## The Current Reality: We Haven't Reached the Threshold Yet

### SWE-bench Performance Shows We're Still in Transition

Current performance data on [SWE-bench](https://www.swebench.com/index.html) reveals where we stand in AI software engineering capabilities:

**Current Performance Metrics (January 2025)**:
- **SWE-bench Full**: Top performers achieve ~20-33% success rate


This represents remarkable progress from 1.96% in 2023, but it also means **our best AI systems still struggle with 50-80% of real-world software engineering challenges**. We are clearly in a transition period, not at the finish line.

### What This Means for Organizations

These numbers have direct implications for software development teams:

1. **AI is genuinely helpful** for many development tasks, accelerating initial implementation and exploration
2. **AI still fails on the majority** of complex, real-world engineering problems
3. **We're in a critical learning phase** where human expertise is needed to improve AI capabilities
4. **Premature workforce reduction** could slow progress toward better AI performance

### The Path to Better SWE Performance

The evidence suggests that reaching higher SWE-bench performance requires exactly what many organizations are eliminating: experienced developers providing validation and feedback.


## Why We Need Experienced Developers for AI Improvement: The Reinforcement learning from human feedback(RLHF) Reality

### The Scaling Challenge

**RLHF Requirements at Current Volume**:
With [41% of code being AI-generated](https://www.elitebrains.com/blog/aI-generated-code-statistics-2025), the need for quality human feedback has never been greater:

- **Quality Assessment**: Each piece of AI-generated code needs expert evaluation for architecture, security, and maintainability
- **Pattern Recognition**: Experienced developers must identify which AI outputs represent good practices worth reinforcing
- **Feedback Loop Management**: Human experts need to provide consistent, high-quality feedback to improve AI training

**The Resource Mismatch**:
- **AI Generation Capacity**: Exponentially increasing (256 billion lines in 2024)
- **Human Validation Capacity**: Decreasing due to layoffs
- **Result**: Widening gap between generation and validation

### The Validation Bottleneck

**Current AI Limitations Create Validation Needs**:
- [SWE-bench performance shows 50-80% of complex problems still require human expertise](http://www.swebench.com/)
- [Security vulnerability rates are 40% higher](https://blog.gitguardian.com/github-copilot-security-and-privacy/) in AI-assisted code
- Each limitation requires human oversight until AI capabilities improve

**The Transition Period Demands More Resources**:
- **Code Generation**: AI tools accelerate output
- **Quality Assurance**: Still requires human expertise  
- **Training Data Curation**: Needs expert judgment to maintain quality
- **Architecture Decisions**: Complex system design still needs human insight

### Why Layoffs Make the Problem Worse

**Reduced Validation Capacity When We Need More**:
Companies are eliminating the exact expertise needed to:
- Validate the massive volume of AI-generated code
- Provide quality feedback for RLHF processes
- Prevent negative feedback loops in training data(when AI codes gets added in future training sets, which is happening in public repository like Github)
- Maintain software engineering standards during transition

### Developers Who Have Lost Jobs: would be Supporting AI for validation


**Everyone Here Supports AI Success**:
- AI technology is genuinely remarkable and transformational
- The capabilities we're seeing today would have seemed impossible just years ago
- AI will eventually transform everything how we build software—and that's exciting

**But Supporting AI Means Protecting Its Development**:
The threshold for full automation simply hasn't been reached yet, and **rushing ahead without proper validation threatens AI's future success**.

### Developers are Protecting AI's Future, Not Hindering It

**Developers are like AI Quality Assurance**:
- **Expert Testing**: Their ability to identify edge cases, integration issues, and system-level problems
- **Quality Standards**: Their understanding of what makes code maintainable, secure, and performant
- **Real-world Validation**: Their experience with how code behaves in production environments
- **Feedback Quality**: Their insights help AI systems learn what constitutes good software engineering

**They are More Valuable Now, Not Less**: The massive volume of AI-generated code creates unprecedented demand for expert testing and validation—exactly the core competencies.

**The Real Threat to AI**: Data corruption from insufficient validation, not human expertise. Understanding that AI needs quality data and expert guidance to reach its potential.

**The Scale of Recent Changes**:
- **2024**: [At least 95,000 workers at U.S.-based tech companies were laid off](https://news.crunchbase.com/startups/tech-layoffs/) in mass job cuts
- **2025 (ongoing)**: [Over 130,000 tech workers have lost jobs across 434 layoff events](https://www.finalroundai.com/blog/ai-tech-layoffs-mid-2025) as of July 2025
- **AI Integration**: Companies cite "realigning workforce to focus on AI" as primary reasons

### AI is Working, But We're Not at the Threshold Yet

**Current AI Capabilities Are Impressive**:
- [SWE-bench performance shows AI handles 20-50% of real-world software engineering challenges successfully](https://www.swebench.com/index.html)
- AI systems can generate functional code for many common programming tasks
- Productivity gains are real for developers using AI tools as assistants

**But We Haven't Reached Full Autonomy**:
- 50-80% of complex, real-world problems still require human expertise
- AI excels at code generation but needs human guidance for architecture and design decisions
- Quality assurance and system integration still require experienced oversight


### The Critical Need: Making AI Better Through Human Feedback

**The Volume of AI-Generated Code**:
Current data shows the scale of AI involvement in software development:
- **[41% of all code is now AI-generated](https://www.elitebrains.com/blog/aI-generated-code-statistics-2025)** according to recent industry analysis
- **[256 billion lines of code](https://www.elitebrains.com/blog/aI-generated-code-statistics-2025)** have been generated by AI as of 2024
- **[63% of professional developers](https://www.gitclear.com/ai_assistant_code_quality_2025_research)** currently use AI in their development process
- **GitHub repositories** show significant AI-generated content, with major portions of new commits involving AI assistance

**Why This Creates an Opportunity, Not a Problem**:
This massive volume of AI-generated code represents an incredible opportunity to improve AI systems, but **only if we have experienced developers providing validation and feedback**.

### The Feedback Loop Challenge

**Making AI Better Requires Human Expertise**:
- **RLHF (Reinforcement Learning from Human Feedback)** needs experienced developers to evaluate code quality
- **Training Data Quality**: With 41% of code being AI-generated, we need human experts to ensure high-quality patterns are preserved
- **Preventing Negative Feedback Loops**: Without human validation,**AI systems risk training on their own imperfect outputs**

**The Real Risk**: If AI trains on AI-generated code without human validation, it creates a negative feedback loop where:
1. AI generates code with subtle flaws
2. This code gets committed to repositories  
3. Future AI systems train on this flawed code
4. The next generation of AI inherits and amplifies these flaws

### Why Developers Expertise is Essential Right Now

**Not to Replace AI, But to Improve It**:
Developers role isn't to compete with AI, but to guide its development:
- **Quality Validation**: Experienced developers can identify architectural issues, security vulnerabilities, and maintainability problems
- **Pattern Recognition**: Developers understanding of good software design helps train AI to generate better code
- **Edge Case Handling**: Developers experience with complex, real-world systems helps AI learn to handle difficult scenarios

### The Business Reality: Strong Profits, Strategic Bets

Many companies making layoffs are simultaneously reporting strong financial performance, suggesting these decisions were strategic bets on future AI capabilities rather than immediate necessity:

- **Microsoft**: [Cut 15,000+ jobs in 2025 while reporting $70.1 billion revenue (13% increase)](https://www.finalroundai.com/blog/ai-tech-layoffs-mid-2025)
- **Intel**: [Laid off 15,000+ employees](https://techcrunch.com/2025/07/31/tech-layoffs-2025-list/) while investing billions in AI infrastructure  
- **Meta**: Cut thousands while posting strong earnings and increasing AI investment

**This Indicates**: Companies believe AI will eventually reach the threshold, but the timing of layoffs preceded the technology being fully ready.

### The Path Forward: Collaboration, Not Replacement

**The Optimal Approach**:
Rather than rushing to full automation, the most successful path forward involves:
1. **AI as powerful assistant**: Use AI for code generation and routine tasks
2. **Human oversight and validation**: Experienced developers ensure quality and provide feedback
3. **Continuous improvement**: Human insights help train better AI systems
4. **Gradual transition**: Increase AI autonomy as capabilities improve and prove reliable

**Why This Benefits Everyone**:
- **Better AI**: Human feedback creates more capable, reliable AI systems
- **Better Code**: Combining AI speed with human expertise produces higher quality results
- **Better Business Outcomes**: Organizations get both productivity gains and quality assurance

### Developer's Situation is Part of a Larger Transition

**The Threshold Will Be Reached**: AI capabilities will eventually justify more automation, but we're not there yet for complex, real-world software engineering.

**Developer's Skills Remain Essential**: The same expertise that made  valuable before is exactly what's needed to guide AI development during this critical transition period.

## The Critical Problem: AI-Generated Code is Polluting Training Data

**A fundamental problem is emerging that makes experienced developer validation even more urgent: AI-generated code is flooding repositories at an unprecedented rate, creating a feedback loop where future AI systems train on the output of previous AI systems. This represents a form of "data pollution" that could degrade AI performance over time.**

### Evidence of Current Data Pollution

Recent research reveals concerning trends:

**Security Vulnerability Rates**: Repositories with Copilot enabled show a 6.4% rate of leaked secrets, which is 40% higher than the 4.6% observed across all public repositories. This suggests that AI-generated code may "inherently contain more security vulnerabilities" or that "the use of coding assistants may be pushing developers to prioritize productivity over code quality and security."

**Training Data Contamination**: GitHub Copilot was initially trained on "a filtered dataset of 159 gigabytes of Python code sourced from 54 million public GitHub repositories". As AI-generated code floods these same repositories, future training cycles will increasingly learn from AI output rather than human-written code.

**Pattern Degradation**: AI systems learn from "all the security failings added to all known public codebases" and "the data it is trained on is also aging rapidly and can't keep up with the latest advances in threats and vulnerabilities".

### Major Code AI Generators and Their Training Sources

Understanding the scope of this problem requires examining the major AI coding tools and their data sources:

#### GitHub Copilot (Microsoft/OpenAI)
- **Training Data**: [Trained on "natural language text and source code from publicly available sources, including code in public repositories on GitHub"](https://github.com/features/copilot)
- **Scale**: [54 million public software repositories with 179 gigabytes of unique Python files](https://medium.com/aggregate-intellect/github-copilot-data-collection-training-and-evaluation-for-large-scale-code-generation-6c1970993998)
- **Model**: Initially used OpenAI Codex, now uses multiple models including GPT-4o
- **Reach**: Most widely adopted AI developer tool with millions of individual users

#### Amazon CodeWhisperer/Q Developer
- **Training Data**: Machine learning-powered code companion trained on publicly available code
- **Focus**: Optimized for AWS services and APIs
- **Integration**: Built into AWS ecosystem and popular IDEs

#### Anthropic Claude 3.5/3.7 Sonnet  
- **Capabilities**: [Achieves "state-of-the-art performance on SWE-bench Verified"](https://www.anthropic.com/news/claude-3-7-sonnet) and excels in "handling complex codebases to advanced tool use"
- **Training**: Details not fully disclosed, but trained on publicly available code repositories
- **Integration**: [Now available in GitHub Copilot](https://github.blog/news-insights/product-news/bringing-developer-choice-to-copilot/), expanding its reach

#### Google Gemini 1.5 Pro
- **Features**: Two-million-token context window and natively multi-modal capabilities
- **Integration**: Available through GitHub Copilot and other platforms

### The Exponential Pollution Problem

This creates a compounding crisis:

1. **Volume**: AI tools generate code faster than humans can properly review it
2. **Quality**: Generated code often appears functional(Also not) but contains architectural or security flaws
3. **Training Contamination**: This code gets committed to public repositories that become training data for future AI models
4. **Cycle Acceleration**: Each generation of AI trains on increasingly AI-generated rather than human-expert content

### Why This Makes RLHF Urgent

The data pollution problem makes experienced developer feedback critical **right now**:

### The Critical Data Quality Imperative

**Stopping Degradation Requires Active Intervention**:
The [massive volume of AI-generated code](https://www.elitebrains.com/blog/aI-generated-code-statistics-2025) means that without sufficient human validation:

1. **Quality Standards Erode**: Unvalidated AI code becomes the new baseline
2. **Training Data Degrades**: Future AI learns from previous AI mistakes
3. **Negative Feedback Loops**: Each generation potentially worse than the last
4. **Recovery Becomes Expensive**: Fixing degraded systems costs more than preventing degradation

**The Window**: 
- We can still shape AI development through quality human feedback, rather than by laying off tech workers
- Training data quality can still be preserved with sufficient validation
- Standards can be maintained if we act during this critical transition period

**The Alternative is Costly**:
- Organizations that reduce validation capacity now may need to completely rebuild AI training datasets later
- Systems built on degraded data may require expensive overhauls, and classification models might not work because AI-generated code, while syntactically correct, may not be functional.
- Competitive disadvantage against organizations that maintained quality standards

## Learning from Other Domains: Why Validation Matters

### The Pattern Across AI Applications

Even Nobel Prize-winning AI tools demonstrate the need for extensive human validation and reveal consistent patterns of generate-many-test-few approaches across scientific domains:

### AlphaFold: PDB-Dependent Structure Prediction with Critical Limitations

[AlphaFold represents one of the greatest computational biology breakthroughs](https://www.nature.com/articles/s41586-021-03819-2), yet fundamental constraints reveal the ongoing need for expert interpretation:

**Core Limitations**:
- Built entirely on existing [PDB structures](https://www.nature.com/articles/s41586-021-03819-2), AlphaFold can only predict structures similar to experimentally determined ones, creating systematic bias toward stable, crystallizable proteins
- [Loop prediction accuracy decreases dramatically with length](https://www.mdpi.com/2218-273X/12/7/985) - loops >20 residues have average RMSD of 2.04 Å compared to 0.33 Å for loops <10 residues
- [pLDDT scores fail to distinguish prediction accuracy](https://www.mdpi.com/2218-273X/12/7/985) for loop regions, leading to false confidence in functionally critical areas
- [AlphaFold2 shows tendency to predict folded states with high pLDDT scores](https://alphafold.ebi.ac.uk/faq) for intrinsically disordered regions that only fold upon binding

**Experimental Validation Gap**: Many instances exist where [predicted structures or parts of predicted structures do not agree with experimentally resolved data](https://www.nature.com/articles/s41592-023-02087-4), even for very high-confidence predictions (pLDDT > 90). [AlphaFold predictions should be considered as exceptionally useful hypotheses that accelerate but do not replace experimental structure determination](https://www.nature.com/articles/s41592-023-02087-4).

### RFdiffusion: The Generate-Thousands-Test-Few Reality in Protein Engineering

[RFdiffusion exemplifies the harsh mathematics of protein design](https://www.nature.com/articles/s41586-023-06415-8), revealing dramatically different success rates depending on task complexity:

**Binder Design Success**: For the benchmark [p53-MDM2 binder design: out of 96 designs, 55 showed some detectable binding at 10 μM](https://www.nature.com/articles/s41586-023-06415-8) - a 57% hit rate that represents unusually high success in protein engineering.

**Beyond Binders: The Enzyme Design Challenge**: While binders are relatively **easier targets(like docking)**, enzyme design represents a much greater challenge requiring precise catalytic activity. The original RFdiffusion [showed only in silico success for enzyme active site scaffolding](https://www.nature.com/articles/s41586-023-06415-8), requiring fine-tuning and demonstrating limitations in explicitly modeling substrates and reaction chemistry. I witnessed this firsthand when I attended Baker's talk at IUPAB 2024 held in Nagoya Japan.

**RFdiffusion2's Improvement but Persistent Challenges**: The newer [RFdiffusion2 made significant progress in enzyme design, successfully producing active enzymes for five distinct chemical reactions by testing fewer than 100 designs per case](https://www.ipd.uw.edu/2025/04/introducing-rfdiffusion2/). For retroaldolase design specifically, [96 designs were expressed and tested, with 4 showing detectable activity](https://www.biorxiv.org/content/10.1101/2025.04.09.648075v1.full) - a 4.2% experimental success rate for selected targets, dramatically lower than the 57% achieved for simple binder design.

**The Fundamental Challenge**: Previously, [tens of thousands of designed molecules had to be tested before finding a single one that performs as intended](https://www.ipd.uw.edu/2022/12/a-diffusion-model-for-protein-design/). While RFdiffusion improves this ratio, the underlying sequence-structure-function relationship remains poorly understood.

### Bhardwaj's Computational Peptide Design: High Accuracy but Limited Scope

Gaurav Bhardwaj's group represents another approach to the peptide design challenge, focusing on [computationally designed hyperstable constrained peptides and macrocycles](https://www.nature.com/articles/nature19791). Their work demonstrates both the potential and limitations of current computational methods.

**Success in Controlled Systems**: Bhardwaj's team achieved remarkable success in designing [macrocyclic peptides with high structural accuracy](https://www.science.org/doi/10.1126/science.aap7577), but the numbers reveal the generate-many-test-few reality. From [more than 200 computationally designed macrocycles, only 12 were selected for experimental testing](https://www.science.org/doi/10.1126/science.aap7577), with [9 out of 12 (75%) showing NMR structures close to computational models](https://www.science.org/doi/10.1126/science.aap7577). Their recent [RFpeptides tool](https://www.ipd.uw.edu/2024/11/introducing-rfpeptides-ai-for-cyclic-peptide-design/) showed similar patterns: ["over a dozen designed macrocycle binders" were synthesized and tested for four target proteins](https://www.ipd.uw.edu/2024/11/introducing-rfpeptides-ai-for-cyclic-peptide-design/), but the total number generated computationally was not disclosed.

**The Numbers Game**: Even in Bhardwaj's highly successful work, the ratio tells the story - 200+ designs generated, 12 tested, 9 successful. This represents a ~94% filtering rate before experimental testing, followed by a 75% experimental success rate, yielding an overall pipeline success of approximately 4.5%.

### MatterGen: Computational Validation and the 15.5% Error Reality

[Microsoft's MatterGen, published in Nature 2025](https://www.nature.com/articles/s41586-025-08628-5), achieved mixed experimental results, but the numbers reveal the harsh reality of computational-to-experimental success. From [8,192 candidates generated for each target bulk modulus value, multiple filtering rounds narrowed this down to 75 candidates, from which 4 were selected for experimental synthesis](https://www.nature.com/articles/s41586-025-08628-5). [Synthesis was successful for only 1 of the 4 candidates](https://www.nature.com/articles/s41586-025-08628-5). For [TaCr2O6 designed with target bulk modulus of 200 GPa, experimental measurement yielded 169 GPa](https://www.nature.com/articles/s41586-025-08628-5) - a relative error of 15.5%.

**The Numbers Game**: MatterGen's pipeline reveals a ~99% filtering rate: 8,192 generated → 75 computationally filtered → 4 selected for synthesis → 1 successful synthesis. This represents an overall pipeline success rate of approximately 0.012% (1 successful out of 8,192 generated).

**Critical Limitations**:
- Material exhibited compositional disorder between Ta and Cr atoms, different from the ordered structure predicted
- Bulk modulus estimated from nanoindentation measurements (158 ± 11 GPa) with maximum value of 169 GPa used as best estimate
- Validation relies primarily on computational methods rather than experimental synthesis

### The Broader Pattern: Sophisticated Pattern Matching vs. Scientific Understanding

**What These Systems Actually Do**: Current AI systems excel at interpolating within existing data but struggle with true extrapolation. They function as pattern recognition tools rather than physics-based predictors:

- **AlphaFold**: Maps sequence patterns to structural patterns from PDB training data
- **RFdiffusion**: Generates structurally plausible designs without understanding functional mechanisms  
- **MatterGen**: Interpolates materials properties within known chemical space
- **Bhardwaj's tools**: Create designs based on learned structural patterns but require extensive experimental validation

**The Experimental Bottleneck**: Overall design success rate remains low despite deep learning improvements. The fundamental issue is generating hypotheses faster than experimental validation capacity, creating resource allocation challenges and confirmation bias in reported results.

### The Critical Difference: Established vs. Missing Validation Pipelines

**Why Software Engineering Faces Unique Risks**: Unlike protein and materials design, which have established experimental and theoritical validation pipelines (molecular modelling,protein expression, crystallography, materials synthesis, property testing), software engineering lacks systematic validation mechanisms for AI-generated code. In scientific domains, every AI prediction must ultimately be tested against physical reality—proteins must fold correctly, materials must exhibit predicted properties, and failed designs are quickly identified and filtered out. However, AI-generated code can appear functional, pass basic tests, and be committed to repositories without comprehensive validation of architecture quality, security implications, or long-term maintainability. This creates a dangerous asymmetry: while scientific AI tools generate thousands of hypotheses knowing most will fail experimental validation, software AI tools generate code that often bypasses rigorous testing and gets directly integrated into production systems and training datasets. The absence of mandatory "experimental validation" for code means that unverified AI-generated software can contaminate repositories, influence future AI training, and compound errors across the entire software development ecosystem—making experienced developer validation not just helpful, but absolutely critical to prevent systemic degradation of software quality.

### The Key Insight for Software Engineering

If domains with **physical validation mechanisms** (chemistry synthesis, protein expression, materials testing) still require extensive human expertise and show significant failure rates, **software engineering - which lacks such validation mechanisms - needs even more human oversight during the AI transition**.

### What This Teaches Us About AI Validation

These examples from AlphaFold, Baker lab tools, and MatterGen illustrate several crucial principles:

1. **Even celebrated tools require extensive validation**: Nobel Prize-winning achievements still need expert interpretation and experimental verification
2. **Transparency about limitations enables progress**: Honest assessment of constraints allows appropriate use and continued improvement  
3. **Success doesn't eliminate the need for expertise**: Revolutionary capabilities enhance rather than replace the need for domain knowledge
4. **The generate-many, validate-few pattern**: All successful AI scientific tools require generating numerous candidates to find a few that work experimentally
5. **Experimental validation remains the gold standard**: Despite computational sophistication, physical reality testing remains essential




### The Promising Yet Challenging Path Forward

The progress is genuinely encouraging, but the computational requirements for marginal improvements are substantial. Recent high-performing approaches achieve better results through increased computational resources rather than fundamental algorithmic breakthroughs, suggesting we may be approaching certain practical limits with current methodologies.

## Conclusion: A Practical Path Forward

### The Current Situation

We find ourselves in a unique moment in technological history:
- **AI capabilities are advancing rapidly** but current SWE-bench performance shows we're still in transition
- **Human expertise is essential** for validation, feedback, and handling the majority of complex problems AI cannot yet solve
- **The improvement path forward** requires exactly what many organizations are eliminating: experienced developers providing RLHF and validation

### The possible path

This isn't an argument against AI adoption—it's an argument for smart transition management:

**Current**: AI as powerful assistant with human validation and feedback
- Use AI for suitable tasks while maintaining human oversight
- Invest in RLHF and validation processes led by experienced developers
- Build hybrid workflows that leverage both AI and human capabilities

**Future**: When SWE-bench full performance reaches 80-90% consistency
- Gradually increase AI autonomy based on demonstrated reliability
- Maintain human expertise for complex edge cases and system architecture
- Continue feedback loops to maintain and improve AI performance

### The Bottom Line

The data suggests we need experienced developers now more than ever as AI progress, but as essential partners in reaching reliable AI capabilities. Organizations that maintain this expertise during the transition period will likely achieve better outcomes than those that rush toward full automation before the technology is ready.

The question isn't whether AI will transform software development—it already has. The question is whether we can manage this transformation thoughtfully, preserving the human expertise needed to guide AI toward truly reliable performance. **The message to big tech companies should be clear: reach 90% success rates on SWE-bench Full before implementing mass layoffs, ensuring AI capabilities truly match the workforce decisions being made.** If 256 billion lines of untested AI-generated code are added to training datasets without proper verification, they may corrupt future AI training and create a negative feedback loop where each generation of AI learns from the unvalidated outputs of previous systems.

**The choice is in our hands: rushed automation with current limitations, or thoughtful collaboration that helps AI reach its full potential.**

**I Support and Stand for Developers.** 

---
